import sys
import os
import time
import argparse
import socket  # ã€è¡¥ä¸ 1ã€‘ å¿…é¡»å¯¼å…¥
import struct  # ã€è¡¥ä¸ 1ã€‘ å¿…é¡»å¯¼å…¥
from pyflink.common import Types, WatermarkStrategy, Duration, Configuration
from pyflink.datastream import StreamExecutionEnvironment, DataStream, OutputTag
from pyflink.common.watermark_strategy import TimestampAssigner
from pyflink.common.time import Time
from pyflink.datastream.window import TumblingEventTimeWindows
from pyflink.datastream.functions import ProcessWindowFunction, MapFunction, RuntimeContext

# ==================== âš™ï¸ å…¨å±€é…ç½®åŒº ====================
PARALLELISM = 3
SERVER_HOST = '192.168.6.205'  # âš ï¸ ç¡®ä¿ä½ çš„ TimeServer ç›‘å¬çš„æ˜¯è¿™ä¸ª IP
SERVER_PORT = 9999
WINDOW_SIZE_MS = 2000
LAG = 5000
EXPERIMENT_NAME = "3p5klag"

# ================= ğŸ“ å…¨å±€å˜é‡ =================
GLOBAL_TIME_OFFSET = 0.0
# åªå®šä¹‰åŸºç¡€ç›®å½•ï¼Œå…·ä½“æ–‡ä»¶ç”±å„ä¸ª Task è‡ªå·±ç”Ÿæˆï¼Œé˜²æ­¢å†²çª
BASE_LOG_DIR = "/tmp/experiment_logs"
# ã€å…³é”®ä¿®å¤ã€‘è¿™é‡Œè¡¥ä¸Šäº†ç¼ºå¤±çš„ Tag å®šä¹‰
LATE_DATA_TAG = OutputTag("late-data", Types.TUPLE([Types.LONG(), Types.INT(), Types.STRING()]))
# ================= ğŸ”§ æ—¶é—´åŒæ­¥å‡½æ•° =================
def sync_time_with_master(master_ip, port=9998):
    offsets = []
    print(f"ğŸ”„ [Sync] æ­£åœ¨ä¸ Master ({master_ip}) åŒæ­¥æ—¶é’Ÿ...")
    for _ in range(5):
        try:
            client = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            client.settimeout(2)
            t1 = time.time()
            client.connect((master_ip, port))
            data = client.recv(8)
            t_server = struct.unpack('!d', data)[0]
            t2 = time.time()
            client.close()
            rtt = t2 - t1
            latency = rtt / 2
            offset = t_server + latency - t2
            offsets.append(offset)
            time.sleep(0.1)
        except Exception as e:
            print(f"âš ï¸ åŒæ­¥å¤±è´¥: {e}")
            return 0.0

    if len(offsets) > 2:
        offsets.remove(max(offsets))
        offsets.remove(min(offsets))

    avg_offset = sum(offsets) / len(offsets)
    print(f"âœ… [Sync] åŒæ­¥å®Œæˆ! æœ¬æœºåç§»é‡: {avg_offset:.6f}s")
    return avg_offset


def get_synced_time():
    return time.time() + GLOBAL_TIME_OFFSET


# ================= ğŸ”§ æ ¸å¿ƒé€»è¾‘ç±» =================

class LogTimestampAssigner(TimestampAssigner):
    def extract_timestamp(self, value, record_timestamp):
        return int(value[0])


class AdvancedWindowStats(ProcessWindowFunction):
    def __init__(self, watermark_setting):
        self.watermark_setting = watermark_setting
        self.task_id = -1
        self.file_path = None
        self.file_path_trace = None

    def open(self, runtime_context: RuntimeContext):
        self.task_id = runtime_context.get_index_of_this_subtask()

        # ã€è¡¥ä¸ 2ã€‘ å¿…é¡»åœ¨è¿™é‡Œç”Ÿæˆæ–‡ä»¶åå’Œåˆ›å»ºç›®å½•ï¼
        # å¦åˆ™è¿œç¨‹æœºå™¨ä¼šæŠ¥ FileNotFoundError
        filename = f"{EXPERIMENT_NAME}-window_Tumbling_P{PARALLELISM}_task-{self.task_id}.csv"
        self.file_path = os.path.join(BASE_LOG_DIR, filename)
        filename_trace = f"{EXPERIMENT_NAME}-all-data-trace_P{PARALLELISM}_task-{self.task_id}.csv"
        self.file_path_trace = os.path.join(BASE_LOG_DIR, filename_trace)
        # 1. ç¡®ä¿ç›®å½•å­˜åœ¨ (æ¯å°æœºå™¨éƒ½è¦åš)
        os.makedirs(BASE_LOG_DIR, exist_ok=True)

        # 2. åŒæ­¥æ—¶é—´
        global GLOBAL_TIME_OFFSET
        GLOBAL_TIME_OFFSET = sync_time_with_master(SERVER_HOST, 9998)

        if not os.path.exists(self.file_path_trace):
            try:
                with open(self.file_path_trace, 'w') as f:
                    f.write("task_id,sys_ts,event_ts,log_id,content,status,window_end\n")
            except:
                pass

        # 3. è¡¥å……è¡¨å¤´ (å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨)
        if not os.path.exists(self.file_path):
            try:
                with open(self.file_path, 'w') as f:
                    f.write(
                        "task_id,window_end,trigger_ts,count_actual,count_expected,loss_network,lag_system,current_wm\n")
            except:
                pass


    def process(self, key, context, elements):
        current_wm = context.current_watermark()
        window_end = context.window().end
        trigger_ts = get_synced_time() * 1000

        log_ids = [e[1] for e in elements]
        count_actual = len(elements)

        if count_actual > 0:
            min_id = min(log_ids)
            max_id = max(log_ids)
            count_expected = max_id - min_id + 1
            loss_network = count_expected - count_actual
        else:
            count_expected = 0
            loss_network = 0

        lag_system = trigger_ts - window_end

        try:
            # ã€è¡¥ä¸ 3ã€‘ ä½¿ç”¨ self.file_path
            with open(self.file_path, 'a') as f:
                f.write(
                    f"{self.task_id},"
                    f"{window_end},{int(trigger_ts)},{count_actual},{count_expected},{loss_network},{int(lag_system)},{current_wm}\n")
        except Exception as e:
            print(f"Write Window Metrics Error: {e}")


        try:
            with open(self.file_path_trace, 'a') as f:
                # æ‰¹é‡æ„å»ºå­—ç¬¦ä¸²ï¼Œå‡å°‘ IO æ¬¡æ•° (æ€§èƒ½ä¼˜åŒ–)
                lines = []
                for e in elements:
                    # e çš„ç»“æ„æ˜¯ (event_ts, log_id, content)
                    event_ts, log_id, content = e
                    # æ ¼å¼: task_id, sys_ts, event_ts, log_id, content, STATUS, window_end
                    line = f"{self.task_id},{int(trigger_ts)},{event_ts},{log_id},{content},NORMAL,{window_end}\n"
                    lines.append(line)

                # ä¸€æ¬¡æ€§å†™å…¥æ•´ä¸ªçª—å£çš„æ•°æ®
                f.writelines(lines)
        except Exception as e:
            print(f"Write Trace Error: {e}")
        result = (f"[{self.task_id}-TUMBLING] >>> ğŸªŸ [Win {window_end}] [WM={current_wm}] | Count={count_actual}")
        yield result


class LateDataLogger(MapFunction):
    def __init__(self):
        self.task_id = -1
        self.file_path_trace = None

    def open(self, runtime_context: RuntimeContext):
        self.task_id = runtime_context.get_index_of_this_subtask()

        # ã€è¡¥ä¸ 4ã€‘ Logger ä¹Ÿè¦åšåŒæ ·çš„ç›®å½•æ£€æŸ¥
        filename_trace = f"{EXPERIMENT_NAME}-all-data-trace_P{PARALLELISM}_task-{self.task_id}.csv"
        self.file_path_trace = os.path.join(BASE_LOG_DIR, filename_trace)

        os.makedirs(BASE_LOG_DIR, exist_ok=True)
        # åŒæ­¥æ—¶é—´...
        global GLOBAL_TIME_OFFSET
        GLOBAL_TIME_OFFSET = sync_time_with_master(SERVER_HOST, 9998)

        # è¡¨å¤´é€»è¾‘ (å’Œä¸Šé¢ä¸€æ ·ï¼Œè°å…ˆåˆ›å»ºè°å†™)
        if not os.path.exists(self.file_path_trace):
            try:
                with open(self.file_path_trace, 'w') as f:
                    f.write("task_id,sys_ts,event_ts,log_id,content,status,window_end\n")
            except:
                pass

    def map(self, value):
        event_ts, log_id, content = value
        system_ts = get_synced_time() * 1000
        lag_magnitude = system_ts - event_ts

        try:
            with open(self.file_path_trace, 'a') as f:
                # è¿Ÿåˆ°æ•°æ®æ²¡æœ‰ window_endï¼Œå¡« -1 æˆ–è€… 0
                f.write(
                    f"{self.task_id},{int(system_ts)},{event_ts},{log_id},{content},LATE,-1\n")
        except Exception as e:
            pass

        return f"[{self.task_id}] âš ï¸ [LATE DROP] ID={log_id}"


# ================= ğŸ”§ è¾…åŠ©å‡½æ•° =================
def create_parallel_socket_source(env, host, port, parallelism):
    try:
        j_env = env._j_stream_execution_environment
        j_data_stream = j_env.socketTextStream(host, int(port), '\n', 0)
        return DataStream(j_data_stream)
    except Exception as e:
        print(f"âŒ Error during Java Gateway call: {e}")
        raise e


class SafeParser(MapFunction):
    def map(self, line):
        try:
            parts = line.split(',', 2)
            if len(parts) < 3: return None
            return (int(parts[0]), int(parts[1]), parts[2])
        except:
            return None


# ================= ğŸš€ ä¸»ç¨‹åº =================

def run_job(max_lag_ms=LAG, window_size=WINDOW_SIZE_MS, parallelism=PARALLELISM):
    # ã€è¡¥ä¸ 5ã€‘ åˆ æ‰äº† run_job é‡Œçš„ init_files()ï¼Œå› ä¸ºæ²¡ç”¨ä¸”æœ‰å®³

    config = Configuration()
    config.set_string("pipeline.auto-watermark-interval", "10ms")
    config.set_string("python.fn-execution.bundle.time", "10")
    config.set_string("python.fn-execution.bundle.size", "1")

    env = StreamExecutionEnvironment.get_execution_environment(config)
    env.set_buffer_timeout(10)
    env.set_parallelism(parallelism)

    print(f"ğŸš€ Job å¯åŠ¨: Tumbling Window {window_size}ms, Par={parallelism}, Lag={max_lag_ms}ms")

    ds_raw = create_parallel_socket_source(env, SERVER_HOST, SERVER_PORT, parallelism)
    ds_distributed = ds_raw.rebalance()

    type_info = Types.TUPLE([Types.LONG(), Types.INT(), Types.STRING()])
    parsed_stream = ds_distributed \
        .map(lambda line: line.strip(), output_type=Types.STRING()) \
        .map(SafeParser(), output_type=type_info) \
        .set_parallelism(parallelism) \
        .filter(lambda x: x is not None)

    watermark_strategy = WatermarkStrategy \
        .for_bounded_out_of_orderness(Duration.of_millis(max_lag_ms)) \
        .with_timestamp_assigner(LogTimestampAssigner())

    windowed_stream = parsed_stream \
        .assign_timestamps_and_watermarks(watermark_strategy) \
        .key_by(lambda x: (x[1] % PARALLELISM) * 1001) \
        .window(TumblingEventTimeWindows.of(Time.milliseconds(window_size))) \
        .side_output_late_data(LATE_DATA_TAG) \
        .process(AdvancedWindowStats(max_lag_ms), Types.STRING())

    windowed_stream.print().set_parallelism(parallelism)

    late_stream = windowed_stream.get_side_output(LATE_DATA_TAG)
    late_stream.map(LateDataLogger()).set_parallelism(parallelism)

    env.execute(f"Tumbling_P{parallelism}_S{window_size}ms_LowLatency")


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--lag', type=int, default=LAG)
    parser.add_argument('--window_size', type=int, default=WINDOW_SIZE_MS)
    parser.add_argument('--parallelism', type=int, default=PARALLELISM)

    args, unknown = parser.parse_known_args()

    run_job(max_lag_ms=args.lag,
            window_size=args.window_size,
            parallelism=args.parallelism)