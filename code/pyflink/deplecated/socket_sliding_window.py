import sys
import os
import time
import argparse
from pyflink.common import Types, WatermarkStrategy, Duration
from pyflink.datastream import StreamExecutionEnvironment, DataStream, OutputTag
from pyflink.common.watermark_strategy import TimestampAssigner
from pyflink.common.time import Time
# [å…³é”®ä¿®æ”¹] å¯¼å…¥ SlidingEventTimeWindows
from pyflink.datastream.window import SlidingEventTimeWindows
from pyflink.datastream.functions import ProcessWindowFunction, MapFunction, RuntimeContext

# ==================== âš™ï¸ å…¨å±€é…ç½®åŒº (å¯ä¿®æ”¹) ====================
PARALLELISM = 3  # Job æ•´ä½“å¹¶è¡Œåº¦ (ç°åœ¨æ˜¯ 2)
SERVER_HOST = '172.25.210.154'  # ä½ çš„æœåŠ¡ç«¯ IP
SERVER_PORT = 9999
WINDOW_SIZE_MS = 2000  # çª—å£å¤§å°ï¼š10ç§’ (10000ms)
WINDOW_SLIDE_MS = 2000  # æ»‘åŠ¨æ­¥é•¿ï¼š2ç§’ (2000ms)
LAG = 1000
# =============================================================
EXPERIMENT_NAME = "to-verify-Stu3020Laixin"
# ================= ğŸ“ å…¨å±€é…ç½®ä¸æ–‡ä»¶åˆå§‹åŒ– =================
FILE_WINDOW_METRICS = f"/tmp/experiment_logs/{EXPERIMENT_NAME}-experiment_window_stats_slide_SZ{WINDOW_SIZE_MS}-TP{WINDOW_SLIDE_MS}-P{PARALLELISM}.csv"
FILE_LATE_LOG = f"/tmp/experiment_logs/{EXPERIMENT_NAME}-experiment_late_data_slide_SZ{WINDOW_SIZE_MS}-TP{WINDOW_SLIDE_MS}-P{PARALLELISM}.csv"



# åˆå§‹åŒ– CSV æ–‡ä»¶å¤´ (ä¿æŒä¸€è‡´ï¼Œç”¨äºå†™å…¥ Task ID)
def init_files():
    os.makedirs("/tmp/experiment_logs", exist_ok=True)
    with open(FILE_WINDOW_METRICS, 'w') as f:
        f.write("task_id,window_end,trigger_ts,count_actual,count_expected,loss_network,lag_system,watermark_setting\n")
    with open(FILE_LATE_LOG, 'w') as f:
        f.write("task_id,system_ts,event_ts,log_id,content,lag_magnitude\n")


# å®šä¹‰ä¾§è¾“å‡ºæµæ ‡ç­¾ (Side Output Tag) ç”¨äºæ•è·è¿Ÿåˆ°æ•°æ®
LATE_DATA_TAG = OutputTag("late-data", Types.TUPLE([Types.LONG(), Types.INT(), Types.STRING()]))


# ================= ğŸ”§ æ ¸å¿ƒé€»è¾‘ç±» (RichFunction) =================

class LogTimestampAssigner(TimestampAssigner):
    def extract_timestamp(self, value, record_timestamp):
        return int(value[0])


class AdvancedWindowStats(ProcessWindowFunction):
    """
    é«˜çº§çª—å£ç»Ÿè®¡ï¼šå†™å…¥ metrics_window.csvï¼ŒåŒ…å« Task ID
    """

    def __init__(self, watermark_setting):
        self.watermark_setting = watermark_setting
        self.task_id = -1

    def open(self, runtime_context: RuntimeContext):
        self.task_id = runtime_context.get_index_of_this_subtask()
        # ã€æ–°å¢ã€‘ç¡®ä¿å½“å‰æœºå™¨ï¼ˆæ— è®ºæ˜¯Masterè¿˜æ˜¯Remoteï¼‰éƒ½æœ‰è¿™ä¸ªæ–‡ä»¶å¤¹
        os.makedirs(os.path.dirname(FILE_WINDOW_METRICS), exist_ok=True)

    def process(self, key, context, elements):
        current_wm = context.current_watermark()
        window_end = context.window().end
        trigger_ts = time.time() * 1000  # ç³»ç»Ÿå½“å‰æ—¶é—´(ms)

        log_ids = [e[1] for e in elements]
        count_actual = len(elements)

        if count_actual > 0:
            min_id = min(log_ids)
            max_id = max(log_ids)
            # æ³¨æ„ï¼šè¿™é‡Œçš„ count_expected åœ¨æ»‘åŠ¨çª—å£ä¸‹é€šå¸¸ä¸å‡†ç¡®ï¼Œå› ä¸ºæ•°æ®ä¼šé‡å 
            count_expected = max_id - min_id + 1
            loss_network = count_expected - count_actual
        else:
            count_expected = 0
            loss_network = 0

        lag_system = trigger_ts - window_end

        # 3. å†™å…¥æ–‡ä»¶ (åŒ…å« self.task_id)
        try:
            with open(FILE_WINDOW_METRICS, 'a') as f:
                f.write(
                    f"{self.task_id},"
                    f"{window_end},{int(trigger_ts)},{count_actual},{count_expected},{loss_network},{int(lag_system)},{current_wm}\n")
        except Exception as e:
            print(f"Write Window Metrics Error: {e}")

        # 4. æ§åˆ¶å°ç²¾ç®€æ‰“å° (æ–°å¢ Task ID)
        result = (f"[{self.task_id}-Stu3020Laixin] >>> ğŸªŸ [SLIDING Win {window_end}] [WM={current_wm}] | "
                  f"Count={count_actual} | "
                  f"SysLag={int(lag_system)}ms")  # ç®€åŒ–æ‰“å°ï¼Œé¿å… NetLoss è¯¯å¯¼
        yield result


class LateDataLogger(MapFunction):
    """
    å¤„ç†è¢«ä¸¢å¼ƒçš„è¿Ÿåˆ°æ•°æ®ï¼Œå¹¶å†™å…¥æ–‡ä»¶ï¼ŒåŒ…å« Task ID
    """

    def __init__(self):
        self.task_id = -1

    def open(self, runtime_context: RuntimeContext):
        self.task_id = runtime_context.get_index_of_this_subtask()

    def map(self, value):
        event_ts, log_id, content = value
        system_ts = time.time() * 1000
        lag_magnitude = system_ts - event_ts

        # å†™å…¥æ–‡ä»¶ (åŒ…å« self.task_id)
        try:
            with open(FILE_LATE_LOG, 'a') as f:
                f.write(
                    f"{self.task_id},"
                    f"{int(system_ts)},{event_ts},{log_id},{content},{int(lag_magnitude)}\n")
        except Exception as e:
            print(f"Write Late Metrics Error: {e}")

        # æ‰“å°åˆ°æ§åˆ¶å° (æ–°å¢ Task ID)
        return f"[{self.task_id}] âš ï¸ [LATE DROP] ID={log_id} (Lag={int(lag_magnitude)}ms)"


# ================= ğŸ”§ 5. Socket Source è¾…åŠ©å‡½æ•° =================
def create_parallel_socket_source(env, host, port, parallelism):
    try:
        j_env = env._j_stream_execution_environment
        # SocketTextStream æ˜¯éå¹¶è¡Œçš„ï¼ŒP=1
        j_data_stream = j_env.socketTextStream(host, int(port), '\n', 0)
        return DataStream(j_data_stream)

    except Exception as e:
        print(f"âŒ Error during Java Gateway call: {e}")
        raise e


# ================= ğŸ”§ 6. æ•°æ®å®‰å…¨è§£æç±» (æ¥è‡ªä¸Šæ¬¡æˆåŠŸè¿è¡Œçš„ç‰ˆæœ¬) =================
class SafeParser(MapFunction):
    def map(self, line):
        line = line.strip()
        if not line:
            return None

        try:
            parts = line.split(',', 2)
            if len(parts) < 3:
                return None

            event_ts = int(parts[0].strip())
            log_id = int(parts[1].strip())
            content = parts[2]

            return (event_ts, log_id, content)

        except ValueError as e:
            # æ•è·éæ³•æ•°æ®ï¼Œé¿å… Task å´©æºƒ
            print(f"Skipping record due to bad format (Non-integer field).")
            return None
        except Exception:
            return None


# ================= ğŸš€ ä¸»ç¨‹åº =================

def run_job(max_lag_ms=LAG, window_size=WINDOW_SIZE_MS, slide_step=WINDOW_SLIDE_MS, parallelism=PARALLELISM):
    # æ¯æ¬¡è¿è¡Œå‰åˆå§‹åŒ–æ–‡ä»¶
    # Update globals for init_files to use (or pass them down, but init_files uses globals)
    global FILE_WINDOW_METRICS, FILE_LATE_LOG, PARALLELISM
    PARALLELISM = parallelism
    FILE_WINDOW_METRICS = f"/tmp/experiment_logs/{EXPERIMENT_NAME}-experiment_window_stats_slide_SZ{window_size}-TP{slide_step}-P{parallelism}.csv"
    FILE_LATE_LOG = f"/tmp/experiment_logs/{EXPERIMENT_NAME}-experiment_late_data_slide_SZ{window_size}-TP{slide_step}-P{parallelism}.csv"
    
    init_files()

    env = StreamExecutionEnvironment.get_execution_environment()
    env.set_parallelism(parallelism)

    print(f"ğŸš€ Job å¯åŠ¨: Sliding Window {window_size}ms/{slide_step}ms, Par={parallelism}, Lag={max_lag_ms}ms")

    # --- 1. Source (P=1) ---
    ds_raw = create_parallel_socket_source(env, SERVER_HOST, SERVER_PORT, parallelism)

    # å¼ºåˆ¶åˆ†å‘ï¼šSource ç«¯ P=1ï¼Œrebalance() ä¼šå°†æ•°æ®åˆ†å‘ç»™ä¸‹æ¸¸ P=PARALLELISM çš„ Tasks
    ds_distributed = ds_raw.rebalance()

    # --- 2. è§£æ (P=PARALLELISM) ---
    type_info = Types.TUPLE([Types.LONG(), Types.INT(), Types.STRING()])

    # æ‰€æœ‰çš„ Map æ“ä½œéƒ½åœ¨ P=PARALLELISM ä¸Šè¿›è¡Œ
    parsed_stream = ds_distributed \
        .map(lambda line: line.strip(), output_type=Types.STRING()) \
        .map(SafeParser(), output_type=type_info) \
        .set_parallelism(parallelism) \
        .filter(lambda x: x is not None)

    # --- 3. Watermark ---
    watermark_strategy = WatermarkStrategy \
        .for_bounded_out_of_orderness(Duration.of_millis(max_lag_ms)) \
        .with_timestamp_assigner(LogTimestampAssigner())

    # --- 4. Window + Side Output ---
    # [æ ¸å¿ƒä¿®æ”¹] ä½¿ç”¨ SlidingEventTimeWindows
    windowed_stream = parsed_stream \
        .assign_timestamps_and_watermarks(watermark_strategy) \
        .key_by(lambda x: x[1] % 10 * 1001) \
        .window(SlidingEventTimeWindows.of(Time.milliseconds(window_size),
                                           Time.milliseconds(slide_step))) \
        .side_output_late_data(LATE_DATA_TAG) \
        .process(AdvancedWindowStats(max_lag_ms), Types.STRING())

    # --- 5. å¤„ç†ä¸»æµç»“æœ (æ‰“å°) ---
    windowed_stream.print().set_parallelism(parallelism)

    # --- 6. å¤„ç†è¿Ÿåˆ°æ•°æ®æµ (å†™å…¥æ–‡ä»¶) ---
    late_stream = windowed_stream.get_side_output(LATE_DATA_TAG)
    late_stream.map(LateDataLogger()).set_parallelism(parallelism)  # MapFunction è‡ªåŠ¨ç»§æ‰¿ output_type

    env.execute(f"Sliding_P{parallelism}_S{window_size / 1000}s_D{slide_step / 1000}s")


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--lag', type=int, default=LAG, help='Max Watermark Lag (ms)')
    parser.add_argument('--window_size', type=int, default=WINDOW_SIZE_MS)
    parser.add_argument('--slide_step', type=int, default=WINDOW_SLIDE_MS)
    parser.add_argument('--parallelism', type=int, default=PARALLELISM)
    
    # Flink might pass other arguments, use parse_known_args
    args, unknown = parser.parse_known_args()
    
    run_job(max_lag_ms=args.lag, 
            window_size=args.window_size, 
            slide_step=args.slide_step, 
            parallelism=args.parallelism)